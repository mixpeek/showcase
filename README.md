# Mixpeek Showcase

Real-world examples and demos showcasing Mixpeek's multimodal search capabilities.

## What is Mixpeek?

Mixpeek is a multimodal AI platform that lets you build semantic search experiences across images, videos, audio, and text. These examples demonstrate how to ingest various types of content and create powerful search applications.

## Examples

### ðŸŽ¨ [National Portrait Gallery](./portrait-gallery)

Build a semantic image search engine using the National Gallery of Art's open-access portrait collection.

- **Data Source:** ~15,000 portrait images from the NGA
- **Features:** Natural language search, metadata filtering, visual similarity
- **Try it live:** [https://mxp.co/r/npg](https://mxp.co/r/npg)

[View Example â†’](./portrait-gallery)

---

## Getting Started

Each example includes:
- **Download scripts** - Fetch open-access data from public sources
- **Ingestion scripts** - Upload content to Mixpeek with metadata
- **Live demos** - Try the search experience

### Prerequisites

- Python 3.7+
- Mixpeek API Key ([Sign up for free](https://mixpeek.com))

### Quick Start

1. Clone this repository
2. Navigate to an example directory
3. Follow the README instructions
4. Try the live demo

## Resources

- [Mixpeek Documentation](https://docs.mixpeek.com)
- [Mixpeek Website](https://mixpeek.com)
- [API Reference](https://docs.mixpeek.com/api-reference)

## License

- **Code:** MIT License (see individual examples)
- **Data:** Each example uses openly licensed data - see individual READMEs for attribution requirements

## Contributing

Have an interesting use case? We'd love to see it! Feel free to submit a pull request with your own example.
